{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Final Project For Kaggle (cleaned version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy loading data\n",
    "class LazyLoadDataset(Dataset):\n",
    "    def __init__(self, path, train = True, transform = None):\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        path = path + ('train/' if train else 'test/')\n",
    "\n",
    "        self.pathX = path + 'X/'\n",
    "        self.pathY = path + 'Y/'\n",
    "\n",
    "        self.data = os.listdir(self.pathX)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        f = self.data[index]\n",
    "\n",
    "        # X\n",
    "        # read rgb images\n",
    "        img0 = cv2.imread(self.pathX + f + '/rgb/0.png')\n",
    "        img1 = cv2.imread(self.pathX + f + '/rgb/1.png')\n",
    "        img2 = cv2.imread(self.pathX + f + '/rgb/2.png')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        \n",
    "        # read depth\n",
    "        depth = np.load(self.pathX + f + '/depth.npy') / 1000\n",
    "\n",
    "        #read field id\n",
    "        field_id = pkl.load(open(self.pathX + f + '/field_id.pkl', 'rb'))\n",
    "\n",
    "        # Y\n",
    "        if self.train:\n",
    "            Y = np.load(self.pathY + f + '.npy')\n",
    "\n",
    "            return (img0, img1, img2, depth, field_id), Y\n",
    "        else:\n",
    "            return (img0, img1, img2, depth, field_id)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mean and Standard Deviation for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), ])\n",
    "\n",
    "# dataset = LazyLoadDataset('./lazydata/', transform = transform)\n",
    "# train_dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "# def get_mean_std(loader):\n",
    "#     (img0, img1, img2, depth, field_id), Y = next(iter(loader))\n",
    "#     img0_mean = img0.mean([0,2,3])\n",
    "#     img0_std = img0.std([0,2,3])\n",
    "#     img1_mean = img1.mean([0,2,3])\n",
    "#     img1_std = img1.std([0,2,3])\n",
    "#     img2_mean = img2.mean([0,2,3])\n",
    "#     img2_std = img2.std([0,2,3])\n",
    "#     mean, std = (img0_mean + img1_mean + img2_mean) / 3, (img0_std + img1_std + img2_std) / 3\n",
    "#     return mean, std\n",
    "\n",
    "# mean, std = get_mean_std(train_dataloader)\n",
    "# print(\"mean and std: \\n\", mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The calculated mean and std (save running time)\n",
    "mean = torch.Tensor([0.4851, 0.4623, 0.4356])\n",
    "std = torch.Tensor([0.2195, 0.2181, 0.2339])\n",
    "\n",
    "# mean and std for img0 only\n",
    "# mean = torch.Tensor([0.4352, 0.4170, 0.3960])\n",
    "# std = torch.Tensor([0.1997, 0.1991, 0.2120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_with_normalization = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std),])\n",
    "dataset = LazyLoadDataset('./lazydata/', transform = transform_with_normalization)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture (Slightly modified from AlexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, conv_feature, fc_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(conv_feature * 6 * 6, fc_feature)\n",
    "        self.fc2 = nn.Linear(fc_feature, fc_feature)\n",
    "        self.fc3 = nn.Linear(fc_feature, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "\n",
    "    Args:\n",
    "        epoch (int): current epoch\n",
    "        model (nn.Module): model to train\n",
    "        optimizer (torch.optim): optimizer to use\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, ((img0, img1, img2, depth, field_id), target) in enumerate(train_dataloader):\n",
    "        # send three images and depth to device\n",
    "        data = torch.cat((img0, img1, img2, depth), dim=1).to(device)\n",
    "\n",
    "        # only img0 input\n",
    "        # data = img0.to(device)\n",
    "        # send target to device\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        # mseloss\n",
    "        loss = nn.MSELoss()(output.float(), target.float() * 1000.0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "            100. * batch_idx / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3396 (0%)]\tLoss: 4588.272461\n",
      "Train Epoch: 0 [640/3396 (19%)]\tLoss: 1140.692017\n",
      "Train Epoch: 0 [1280/3396 (37%)]\tLoss: 687.662964\n",
      "Train Epoch: 0 [1920/3396 (56%)]\tLoss: 622.188232\n",
      "Train Epoch: 0 [2560/3396 (74%)]\tLoss: 581.894165\n",
      "Train Epoch: 0 [3200/3396 (93%)]\tLoss: 497.133240\n",
      "Train Epoch: 1 [0/3396 (0%)]\tLoss: 531.858643\n",
      "Train Epoch: 1 [640/3396 (19%)]\tLoss: 537.068481\n",
      "Train Epoch: 1 [1280/3396 (37%)]\tLoss: 546.668640\n",
      "Train Epoch: 1 [1920/3396 (56%)]\tLoss: 528.822571\n",
      "Train Epoch: 1 [2560/3396 (74%)]\tLoss: 500.691162\n",
      "Train Epoch: 1 [3200/3396 (93%)]\tLoss: 504.248657\n",
      "Train Epoch: 2 [0/3396 (0%)]\tLoss: 497.456299\n",
      "Train Epoch: 2 [640/3396 (19%)]\tLoss: 484.272522\n",
      "Train Epoch: 2 [1280/3396 (37%)]\tLoss: 532.355591\n",
      "Train Epoch: 2 [1920/3396 (56%)]\tLoss: 496.879089\n",
      "Train Epoch: 2 [2560/3396 (74%)]\tLoss: 496.472015\n",
      "Train Epoch: 2 [3200/3396 (93%)]\tLoss: 392.465332\n",
      "Train Epoch: 3 [0/3396 (0%)]\tLoss: 290.215637\n",
      "Train Epoch: 3 [640/3396 (19%)]\tLoss: 251.339569\n",
      "Train Epoch: 3 [1280/3396 (37%)]\tLoss: 233.318588\n",
      "Train Epoch: 3 [1920/3396 (56%)]\tLoss: 234.519135\n",
      "Train Epoch: 3 [2560/3396 (74%)]\tLoss: 216.239609\n",
      "Train Epoch: 3 [3200/3396 (93%)]\tLoss: 183.346405\n",
      "Train Epoch: 4 [0/3396 (0%)]\tLoss: 157.649292\n",
      "Train Epoch: 4 [640/3396 (19%)]\tLoss: 163.951050\n",
      "Train Epoch: 4 [1280/3396 (37%)]\tLoss: 160.343781\n",
      "Train Epoch: 4 [1920/3396 (56%)]\tLoss: 141.373962\n",
      "Train Epoch: 4 [2560/3396 (74%)]\tLoss: 127.155457\n",
      "Train Epoch: 4 [3200/3396 (93%)]\tLoss: 119.069489\n",
      "Train Epoch: 5 [0/3396 (0%)]\tLoss: 129.098129\n",
      "Train Epoch: 5 [640/3396 (19%)]\tLoss: 110.867722\n",
      "Train Epoch: 5 [1280/3396 (37%)]\tLoss: 97.337021\n",
      "Train Epoch: 5 [1920/3396 (56%)]\tLoss: 109.014084\n",
      "Train Epoch: 5 [2560/3396 (74%)]\tLoss: 79.932930\n",
      "Train Epoch: 5 [3200/3396 (93%)]\tLoss: 83.072723\n",
      "Train Epoch: 6 [0/3396 (0%)]\tLoss: 111.700699\n",
      "Train Epoch: 6 [640/3396 (19%)]\tLoss: 79.952271\n",
      "Train Epoch: 6 [1280/3396 (37%)]\tLoss: 82.936783\n",
      "Train Epoch: 6 [1920/3396 (56%)]\tLoss: 75.615471\n",
      "Train Epoch: 6 [2560/3396 (74%)]\tLoss: 73.702347\n",
      "Train Epoch: 6 [3200/3396 (93%)]\tLoss: 78.074341\n",
      "Train Epoch: 7 [0/3396 (0%)]\tLoss: 83.635239\n",
      "Train Epoch: 7 [640/3396 (19%)]\tLoss: 69.242195\n",
      "Train Epoch: 7 [1280/3396 (37%)]\tLoss: 71.907349\n",
      "Train Epoch: 7 [1920/3396 (56%)]\tLoss: 66.408417\n",
      "Train Epoch: 7 [2560/3396 (74%)]\tLoss: 66.152252\n",
      "Train Epoch: 7 [3200/3396 (93%)]\tLoss: 60.316235\n",
      "Train Epoch: 8 [0/3396 (0%)]\tLoss: 61.750793\n",
      "Train Epoch: 8 [640/3396 (19%)]\tLoss: 59.721214\n",
      "Train Epoch: 8 [1280/3396 (37%)]\tLoss: 65.800011\n",
      "Train Epoch: 8 [1920/3396 (56%)]\tLoss: 67.086258\n",
      "Train Epoch: 8 [2560/3396 (74%)]\tLoss: 70.422333\n",
      "Train Epoch: 8 [3200/3396 (93%)]\tLoss: 64.957756\n",
      "Train Epoch: 9 [0/3396 (0%)]\tLoss: 64.354942\n",
      "Train Epoch: 9 [640/3396 (19%)]\tLoss: 56.480247\n",
      "Train Epoch: 9 [1280/3396 (37%)]\tLoss: 73.329750\n",
      "Train Epoch: 9 [1920/3396 (56%)]\tLoss: 52.617977\n",
      "Train Epoch: 9 [2560/3396 (74%)]\tLoss: 65.583740\n",
      "Train Epoch: 9 [3200/3396 (93%)]\tLoss: 51.925247\n",
      "Train Epoch: 10 [0/3396 (0%)]\tLoss: 71.996231\n",
      "Train Epoch: 10 [640/3396 (19%)]\tLoss: 63.736305\n",
      "Train Epoch: 10 [1280/3396 (37%)]\tLoss: 52.891891\n",
      "Train Epoch: 10 [1920/3396 (56%)]\tLoss: 59.353516\n",
      "Train Epoch: 10 [2560/3396 (74%)]\tLoss: 60.209198\n",
      "Train Epoch: 10 [3200/3396 (93%)]\tLoss: 55.500175\n",
      "Train Epoch: 11 [0/3396 (0%)]\tLoss: 83.555008\n",
      "Train Epoch: 11 [640/3396 (19%)]\tLoss: 52.271492\n",
      "Train Epoch: 11 [1280/3396 (37%)]\tLoss: 54.885372\n",
      "Train Epoch: 11 [1920/3396 (56%)]\tLoss: 45.250732\n",
      "Train Epoch: 11 [2560/3396 (74%)]\tLoss: 66.354599\n",
      "Train Epoch: 11 [3200/3396 (93%)]\tLoss: 56.087982\n",
      "Train Epoch: 12 [0/3396 (0%)]\tLoss: 55.093151\n",
      "Train Epoch: 12 [640/3396 (19%)]\tLoss: 54.838493\n",
      "Train Epoch: 12 [1280/3396 (37%)]\tLoss: 44.112133\n",
      "Train Epoch: 12 [1920/3396 (56%)]\tLoss: 43.872421\n",
      "Train Epoch: 12 [2560/3396 (74%)]\tLoss: 42.317368\n",
      "Train Epoch: 12 [3200/3396 (93%)]\tLoss: 55.077419\n",
      "Train Epoch: 13 [0/3396 (0%)]\tLoss: 53.937054\n",
      "Train Epoch: 13 [640/3396 (19%)]\tLoss: 50.160324\n",
      "Train Epoch: 13 [1280/3396 (37%)]\tLoss: 44.652569\n",
      "Train Epoch: 13 [1920/3396 (56%)]\tLoss: 38.753151\n",
      "Train Epoch: 13 [2560/3396 (74%)]\tLoss: 41.018414\n",
      "Train Epoch: 13 [3200/3396 (93%)]\tLoss: 41.957405\n",
      "Train Epoch: 14 [0/3396 (0%)]\tLoss: 49.629284\n",
      "Train Epoch: 14 [640/3396 (19%)]\tLoss: 41.636517\n",
      "Train Epoch: 14 [1280/3396 (37%)]\tLoss: 67.532822\n",
      "Train Epoch: 14 [1920/3396 (56%)]\tLoss: 46.471092\n",
      "Train Epoch: 14 [2560/3396 (74%)]\tLoss: 54.520821\n",
      "Train Epoch: 14 [3200/3396 (93%)]\tLoss: 44.980003\n",
      "Train Epoch: 15 [0/3396 (0%)]\tLoss: 49.743923\n",
      "Train Epoch: 15 [640/3396 (19%)]\tLoss: 37.926796\n",
      "Train Epoch: 15 [1280/3396 (37%)]\tLoss: 47.419079\n",
      "Train Epoch: 15 [1920/3396 (56%)]\tLoss: 47.081116\n",
      "Train Epoch: 15 [2560/3396 (74%)]\tLoss: 41.274719\n",
      "Train Epoch: 15 [3200/3396 (93%)]\tLoss: 47.426758\n",
      "Train Epoch: 16 [0/3396 (0%)]\tLoss: 50.182114\n",
      "Train Epoch: 16 [640/3396 (19%)]\tLoss: 39.053436\n",
      "Train Epoch: 16 [1280/3396 (37%)]\tLoss: 44.329674\n",
      "Train Epoch: 16 [1920/3396 (56%)]\tLoss: 40.874454\n",
      "Train Epoch: 16 [2560/3396 (74%)]\tLoss: 44.262848\n",
      "Train Epoch: 16 [3200/3396 (93%)]\tLoss: 39.148899\n",
      "Train Epoch: 17 [0/3396 (0%)]\tLoss: 45.565636\n",
      "Train Epoch: 17 [640/3396 (19%)]\tLoss: 39.448334\n",
      "Train Epoch: 17 [1280/3396 (37%)]\tLoss: 39.006027\n",
      "Train Epoch: 17 [1920/3396 (56%)]\tLoss: 38.481865\n",
      "Train Epoch: 17 [2560/3396 (74%)]\tLoss: 37.055717\n",
      "Train Epoch: 17 [3200/3396 (93%)]\tLoss: 46.795544\n",
      "Train Epoch: 18 [0/3396 (0%)]\tLoss: 38.958645\n",
      "Train Epoch: 18 [640/3396 (19%)]\tLoss: 44.987392\n",
      "Train Epoch: 18 [1280/3396 (37%)]\tLoss: 38.838913\n",
      "Train Epoch: 18 [1920/3396 (56%)]\tLoss: 40.083630\n",
      "Train Epoch: 18 [2560/3396 (74%)]\tLoss: 32.723446\n",
      "Train Epoch: 18 [3200/3396 (93%)]\tLoss: 36.640106\n",
      "Train Epoch: 19 [0/3396 (0%)]\tLoss: 50.682945\n",
      "Train Epoch: 19 [640/3396 (19%)]\tLoss: 39.717857\n",
      "Train Epoch: 19 [1280/3396 (37%)]\tLoss: 39.889801\n",
      "Train Epoch: 19 [1920/3396 (56%)]\tLoss: 32.931149\n",
      "Train Epoch: 19 [2560/3396 (74%)]\tLoss: 45.409798\n",
      "Train Epoch: 19 [3200/3396 (93%)]\tLoss: 35.738003\n"
     ]
    }
   ],
   "source": [
    "# Training settings \n",
    "# number of feature maps\n",
    "conv_features = 256\n",
    "# number of input channels\n",
    "input_channels = 12\n",
    "fc_features = 4096\n",
    "output_size = 12\n",
    "\n",
    "# optimal lr\n",
    "lr = 0.0001\n",
    "\n",
    "# test lr\n",
    "# lr = 0.0001\n",
    "\n",
    "model_cnn = CNN(input_channels, conv_features, fc_features, output_size) # create CNN model\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr = lr) # create optimizer\n",
    "\n",
    "for epoch in range(0, 20):\n",
    "    train(epoch, model_cnn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = LazyLoadDataset('./lazydata/', train = False, transform = transform_with_normalization)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64 * 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict(model):\n",
    "    outfile = 'submission.csv'\n",
    "    output_file = open(outfile, 'w')\n",
    "    titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
    "         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
    "    \n",
    "    model.eval()\n",
    "    pred = []\n",
    "    file_ids = []\n",
    "\n",
    "    for i, ((img0, img1, img2, depth, field_id)) in enumerate(test_dataloader):\n",
    "        data = torch.cat((img0, img1, img2, depth), dim=1).to(device)\n",
    "        # data = img0.to(device)\n",
    "        output = model(data)\n",
    "        pred.append(output.cpu().detach().numpy())\n",
    "        file_ids.extend(field_id)\n",
    "    \n",
    "    pred = np.concatenate(pred) / 1000.0\n",
    "\n",
    "    df = pd.concat([pd.DataFrame(file_ids), pd.DataFrame.from_records(pred)], axis = 1, names = titles)\n",
    "    df.columns = titles\n",
    "    df.to_csv(outfile, index = False)\n",
    "    print(\"Written to csv file {}\".format(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to csv file submission.csv\n"
     ]
    }
   ],
   "source": [
    "predict(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
